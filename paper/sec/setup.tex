
\section{Experiment Setup}

I wrote a simple web crawler in python that crawled the NCBI web interface and downloaded all virus genomes then stored them in the filesystem. Since the filesystem is already physically partitioned into three locations, I decided to have three crawlers running in parallel, one on each of the GEE nodes, and distribute the work so that approximately one third of all the genomes would end up at each location. The end result was that Toronto ended up housing $1864$ genomes, Carlton $1729$, and Victoria $1942$. The split was not entirely symmetrical as the number of links to follow was partitioned, not the genomes themselves, and any given link could contain a genome, multiple genomes, or not contain any at all. The crawler was also used to download the GRCh38 reference sequence from the NCBI database. Since the data was already partitioned into separate locations, each machine in the experiments that follow worked exclusively on virus genome that was present at one given location, and uploaded results back into the filesystem at the same location.